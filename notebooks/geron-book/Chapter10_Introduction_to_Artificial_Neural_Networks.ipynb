{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History of Neural Networks\n",
    "- **Artificial neuron**, [McCulloch, W.S. and Pitts, W., 1943](http://aiplaybook.a16z.com/reference-material/mcculloch-pitts-1943-neural-networks.pdf): Input layer has one or more binary inputs. Output layer has one binary output. Can learn XOR. Different artificial neuron can be combined to compute complex logical expressions.\n",
    "- **Perceptron**, [Rosenblatt 1957](https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf): Input layer has one or more real inputs. Input layer is connected by weights to the output layer (so no hidden layers). Output layer has $P \\in \\mathbb{N}$ output units $h(\\mathbf{x}) = \\mathbb{1}_{\\mathbf{x}^T \\mathbf{w} \\geq 0}$ for $p=1,...,P$ where $\\mathbb{1}$ indicates a step function. Note that $h_i$ is a hard threshold function (i.e. not a logistic function). A unit bias is added to the input layer. Weights are trained online according to\n",
    "$$w_{i, j} \\leftarrow w_{i, j} - \\alpha(\\hat{y}_j - y_j) x_i$$\n",
    "where $w_{i, j}$ refers to the weights connecting $x_i$ to $h_j$ (aka Stochastic Gradient Descent). In [Minsky, M., & Papert, S. (1969)](http://psycnet.apa.org/record/1969-35017-000) the authors showed that Perceptron cannot learn XOR because the decision boundary is linear (just like logistic regression). However, if the decision boundary is linear, the update rule converges to a solution.\n",
    "- **Multi-layer perceptron and backpropagation**: Hidden layers can be added to a Perceptron to solve XOR. However, training was hard until when [DE Rumelhart, GE Hinton, RJ Williams, 1985](https://goo.gl/Wl7Xyc) presented backpropagation (today known as gradient descent using reverse-mode autodiff), a suitable learning algorithm to find $\\mathbf{W}$. Backpropagation: for each training instance\n",
    "    1. Compute $\\hat{y}_j = \\sigma(.)$ where $\\sigma(.)$ is a logistic function (forward pass)\n",
    "    2. Go through each layer in reverse to measure the error contribution from each connection (reverse pass)\n",
    "    3. Update the weights with gradient descent.\n",
    "    \n",
    "Output units can now be transformed with softmax of logistic function, which are suitable differentiable functions for the gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a MLP using plain tensorflow on MNIST\n",
    "The goal is to train a MLP using mini-batch gradient descent on MNIST using the low-level tensorflow API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import cufflinks\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import show_graph\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "cufflinks.go_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load MNIST and split in training, validation and test set.\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Sizes of the dataset.\n",
    "train_size, pixels_rows, pixels_cols = X_train.shape\n",
    "test_size, _, _ = X_test.shape\n",
    "nr_features = pixels_rows * pixels_cols\n",
    "nr_labels = len(np.unique(y_train))\n",
    "\n",
    "# split X_train in smaller X_train and X_valid.\n",
    "split_threshold = int(train_size * 0.7)  \n",
    "X_test = X_test.reshape(test_size, pixels_rows * pixels_cols) / 255.\n",
    "X_train = X_train.reshape(train_size, pixels_rows * pixels_cols) / 255.\n",
    "X_train, X_valid = X_train[:split_threshold], X_train[split_threshold:]\n",
    "y_train, y_valid = y_train[:split_threshold], y_train[split_threshold:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction phase\n",
    "There are a few necessary building block to define our computation graph representing a MLP, one for each of the following sub-sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "A MLP is a function $h : \\mathbf{x} \\in \\mathbb{R}^n \\mapsto \\hat{y} \\in \\mathbb{R}$. For MNIST (classification), input $\\mathbf{x}$ corresponds to an 27x27 image of a hand-written digit and $\\hat{y}$ refers to the predicted label.\n",
    "\n",
    "Ideally, we want our code to be able to handle vectorised operations (i.e. to predict in batches) so $h : X \\in \\mathbb{R}^{b \\times n} \\mapsto \\hat{y} \\in \\mathbb{R} ^ b$ where $b \\in \\mathbb{N}$ indicates the batch size. This generalisation will help training mini-batch gradient descent more efficiently.\n",
    "\n",
    "Setting $b$ to $\\texttt{None}$ allows to provide a flexible number of observations without enforcing a pre-defined number. If we specified a value, say $b = 10$, an exception will be raised whenever the amount observations passed to the MLP differs from 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(dtype=tf.float32, shape=(None, nr_features), name='X')\n",
    "y = tf.placeholder(dtype=tf.int32, shape=(None), name='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network architecture\n",
    "We define a MLP with the following architecture:\n",
    "- Input layer: accepts tensors with any number of overvations (row) and a fixed number of features ($\\texttt{mnist_n_inputs}$).\n",
    "- Hidden layer: 300 units, ReLU\n",
    "- Hidden layer: 100 units, ReLU\n",
    "- Output layer: 10 units, linear. Logits will be passed to softmax in order to map logits to probabilities, which will be needed to define the loss function (cross-entropy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('nn-architecture'):\n",
    "    hidden_layer_1 = tf.layers.dense(\n",
    "        inputs=X,\n",
    "        units=300,\n",
    "        activation=tf.nn.relu,\n",
    "        name='hidden_layer_1',\n",
    "    )\n",
    "    hidden_layer_2 = tf.layers.dense(\n",
    "        inputs=hidden_layer_1,\n",
    "        units=100,\n",
    "        activation=tf.nn.relu,\n",
    "        name='hidden_layer_2',\n",
    "    )\n",
    "    logits = tf.layers.dense(\n",
    "        inputs=hidden_layer_2,\n",
    "        units=nr_labels,\n",
    "        name='output_layer',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "We use the cross-entropy loss to train our MLP defined as\n",
    "\n",
    "$$H(\\hat{p}) =  - \\frac{1}{m} \\sum_{i=1}^m \\sum_{c=1}^C I_{[y_i = c]} \\log_2 \\hat{p}(y_i = c)$$\n",
    "where\n",
    "$$ \\hat{p}(y_i = c) = \\text{softmax}(\\mathbf{z}_{i, c}) = \\frac{e^{\\mathbf{z}_{i, c}}}{\\sum_{c=1}^C e^{\\mathbf{z}_{i, c}}}.$$\n",
    "\n",
    "\n",
    "Having two different functions to compute the cross entropy loss directly from logits is a convenience, as they produce the same result:\n",
    "- $\\texttt{tf.nn.softmax_cross_entropy_with_logits}$: labels must have the shape [batch_size, num_classes] and dtype float.\n",
    "- $\\texttt{tf.nn.sparse_softmax_cross_entropy_with_logits}$: labels must have the shape [batch_size] and the dtype int. Each label is an int in range [0, num_classes-1].\n",
    "\n",
    "Note that in our case batch_size is flexible because it was set to $\\texttt{None}.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('cross_entropy'):\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y,\n",
    "        logits=logits,\n",
    "        name='cross_entropy',\n",
    "    )\n",
    "    cross_entropy_loss = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimiser\n",
    "Here we define the gradient descent to minimise the cross-entropy loss. `minimize` adds operations to minimize `loss` by updating `var_list`. This method simply combines calls to `compute_gradients` and `apply_gradients`. The latter approach allows to applying logic to the gradient before being used to update the weights (e.g. clipping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('gradient_descent'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01, name='gradient_descent')\n",
    "\n",
    "    # You could equivalently use training_op = optimizer.minimize(cross_entropy).\n",
    "    gradients = optimizer.compute_gradients(cross_entropy)\n",
    "    training_op = optimizer.apply_gradients(grads_and_vars=gradients, name='apply_gradient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "Convert logits to $\\hat{y}$ for each observation in the batch. If the labels were originally stored as integer labels, pass them directly to `tf.nn.in_top_k()` without converting them to one-hot.\n",
    "\n",
    "Refernce: [Stackoverflow - TensorFlow in_top_k evaluation input argumants](https://stackoverflow.com/questions/36080445/tensorflow-in-top-k-evaluation-input-argumants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('validation'):\n",
    "    is_correct_label_int = tf.nn.in_top_k(\n",
    "        predictions=logits,\n",
    "        targets=y,\n",
    "        k=1,\n",
    "        name='is_correct_label_vec',\n",
    "    )\n",
    "    is_correct_label_float = tf.cast(is_correct_label_int, tf.float32)\n",
    "    accuracy = tf.reduce_mean(is_correct_label_float, name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation graph\n",
    "Visualise computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.7599694552394674&quot;).pbtxt = 'node {\\n  name: &quot;X&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 784\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;y&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        unknown_rank: true\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_1/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\020\\\\003\\\\000\\\\000,\\\\001\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_1/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.07439795136451721\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_1/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.07439795136451721\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_1/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden_layer_1/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_1/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden_layer_1/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden_layer_1/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_1/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden_layer_1/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden_layer_1/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_1/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden_layer_1/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden_layer_1/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_1/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 784\\n        }\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_1/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden_layer_1/kernel&quot;\\n  input: &quot;hidden_layer_1/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_1/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden_layer_1/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_1/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 300\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_1/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_1/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden_layer_1/bias&quot;\\n  input: &quot;hidden_layer_1/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_1/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden_layer_1/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_1/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;nn-architecture/hidden_layer_1/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;hidden_layer_1/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;nn-architecture/hidden_layer_1/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;nn-architecture/hidden_layer_1/MatMul&quot;\\n  input: &quot;hidden_layer_1/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;nn-architecture/hidden_layer_1/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;nn-architecture/hidden_layer_1/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_2/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;,\\\\001\\\\000\\\\000d\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_2/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.12247448414564133\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_2/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.12247448414564133\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_2/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden_layer_2/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_2/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden_layer_2/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden_layer_2/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_2/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden_layer_2/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden_layer_2/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_2/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden_layer_2/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden_layer_2/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_2/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 300\\n        }\\n        dim {\\n          size: 100\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_2/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden_layer_2/kernel&quot;\\n  input: &quot;hidden_layer_2/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_2/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden_layer_2/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_2/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 100\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_2/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 100\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_2/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden_layer_2/bias&quot;\\n  input: &quot;hidden_layer_2/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden_layer_2/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden_layer_2/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_2/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;nn-architecture/hidden_layer_2/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;nn-architecture/hidden_layer_1/Relu&quot;\\n  input: &quot;hidden_layer_2/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;nn-architecture/hidden_layer_2/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;nn-architecture/hidden_layer_2/MatMul&quot;\\n  input: &quot;hidden_layer_2/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;nn-architecture/hidden_layer_2/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;nn-architecture/hidden_layer_2/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_layer/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;d\\\\000\\\\000\\\\000\\\\n\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_layer/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.23354968428611755\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_layer/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.23354968428611755\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_layer/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;output_layer/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_layer/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;output_layer/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;output_layer/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output_layer/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_layer/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;output_layer/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;output_layer/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output_layer/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_layer/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;output_layer/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;output_layer/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output_layer/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_layer/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 100\\n        }\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_layer/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output_layer/kernel&quot;\\n  input: &quot;output_layer/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_layer/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;output_layer/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output_layer/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_layer/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output_layer/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 10\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_layer/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output_layer/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_layer/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;output_layer/bias&quot;\\n  input: &quot;output_layer/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output_layer/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output_layer/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;output_layer/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output_layer/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;nn-architecture/output_layer/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;nn-architecture/hidden_layer_2/Relu&quot;\\n  input: &quot;output_layer/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;nn-architecture/output_layer/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;nn-architecture/output_layer/MatMul&quot;\\n  input: &quot;output_layer/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;cross_entropy/cross_entropy/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;cross_entropy/cross_entropy/cross_entropy&quot;\\n  op: &quot;SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;nn-architecture/output_layer/BiasAdd&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tlabels&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;cross_entropy/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;cross_entropy/Mean&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;cross_entropy/cross_entropy/cross_entropy&quot;\\n  input: &quot;cross_entropy/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;cross_entropy/cross_entropy/cross_entropy&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/grad_ys_0&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradient_descent/gradients/Shape&quot;\\n  input: &quot;gradient_descent/gradients/grad_ys_0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;index_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/zeros_like&quot;\\n  op: &quot;ZerosLike&quot;\\n  input: &quot;cross_entropy/cross_entropy/cross_entropy:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/cross_entropy/cross_entropy/cross_entropy_grad/PreventGradient&quot;\\n  op: &quot;PreventGradient&quot;\\n  input: &quot;cross_entropy/cross_entropy/cross_entropy:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;message&quot;\\n    value {\\n      s: &quot;Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation\\\\\\'s interaction with tf.gradients()&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/cross_entropy/cross_entropy/cross_entropy_grad/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/cross_entropy/cross_entropy/cross_entropy_grad/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;gradient_descent/gradients/Fill&quot;\\n  input: &quot;gradient_descent/gradients/cross_entropy/cross_entropy/cross_entropy_grad/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/cross_entropy/cross_entropy/cross_entropy_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradient_descent/gradients/cross_entropy/cross_entropy/cross_entropy_grad/ExpandDims&quot;\\n  input: &quot;gradient_descent/gradients/cross_entropy/cross_entropy/cross_entropy_grad/PreventGradient&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/output_layer/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradient_descent/gradients/cross_entropy/cross_entropy/cross_entropy_grad/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/output_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradient_descent/gradients/cross_entropy/cross_entropy/cross_entropy_grad/mul&quot;\\n  input: &quot;^gradient_descent/gradients/nn-architecture/output_layer/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/output_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradient_descent/gradients/cross_entropy/cross_entropy/cross_entropy_grad/mul&quot;\\n  input: &quot;^gradient_descent/gradients/nn-architecture/output_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradient_descent/gradients/cross_entropy/cross_entropy/cross_entropy_grad/mul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/output_layer/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/output_layer/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradient_descent/gradients/nn-architecture/output_layer/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradient_descent/gradients/nn-architecture/output_layer/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/output_layer/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/output_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;output_layer/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/output_layer/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;nn-architecture/hidden_layer_2/Relu&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/output_layer/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/output_layer/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradient_descent/gradients/nn-architecture/output_layer/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradient_descent/gradients/nn-architecture/output_layer/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/output_layer/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/output_layer/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradient_descent/gradients/nn-architecture/output_layer/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradient_descent/gradients/nn-architecture/output_layer/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/output_layer/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/output_layer/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradient_descent/gradients/nn-architecture/output_layer/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradient_descent/gradients/nn-architecture/output_layer/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_2/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/output_layer/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;nn-architecture/hidden_layer_2/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_2/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_2/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_2/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradient_descent/gradients/nn-architecture/hidden_layer_2/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradient_descent/gradients/nn-architecture/hidden_layer_2/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_2/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_2/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradient_descent/gradients/nn-architecture/hidden_layer_2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradient_descent/gradients/nn-architecture/hidden_layer_2/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_2/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradient_descent/gradients/nn-architecture/hidden_layer_2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradient_descent/gradients/nn-architecture/hidden_layer_2/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_2/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_2/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden_layer_2/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_2/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;nn-architecture/hidden_layer_1/Relu&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_2/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_2/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradient_descent/gradients/nn-architecture/hidden_layer_2/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradient_descent/gradients/nn-architecture/hidden_layer_2/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_2/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_2/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradient_descent/gradients/nn-architecture/hidden_layer_2/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradient_descent/gradients/nn-architecture/hidden_layer_2/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_2/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_2/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradient_descent/gradients/nn-architecture/hidden_layer_2/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradient_descent/gradients/nn-architecture/hidden_layer_2/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_1/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_2/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;nn-architecture/hidden_layer_1/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_1/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_1/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_1/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradient_descent/gradients/nn-architecture/hidden_layer_1/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradient_descent/gradients/nn-architecture/hidden_layer_1/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_1/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_1/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradient_descent/gradients/nn-architecture/hidden_layer_1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradient_descent/gradients/nn-architecture/hidden_layer_1/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_1/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradient_descent/gradients/nn-architecture/hidden_layer_1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradient_descent/gradients/nn-architecture/hidden_layer_1/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_1/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_1/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden_layer_1/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_1/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_1/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_1/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradient_descent/gradients/nn-architecture/hidden_layer_1/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradient_descent/gradients/nn-architecture/hidden_layer_1/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_1/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_1/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradient_descent/gradients/nn-architecture/hidden_layer_1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradient_descent/gradients/nn-architecture/hidden_layer_1/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_1/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_1/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradient_descent/gradients/nn-architecture/hidden_layer_1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradient_descent/gradients/nn-architecture/hidden_layer_1/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/apply_gradient/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.009999999776482582\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/apply_gradient/update_hidden_layer_1/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden_layer_1/kernel&quot;\\n  input: &quot;gradient_descent/apply_gradient/learning_rate&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_1/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/apply_gradient/update_hidden_layer_1/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden_layer_1/bias&quot;\\n  input: &quot;gradient_descent/apply_gradient/learning_rate&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/apply_gradient/update_hidden_layer_2/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden_layer_2/kernel&quot;\\n  input: &quot;gradient_descent/apply_gradient/learning_rate&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_2/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/apply_gradient/update_hidden_layer_2/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden_layer_2/bias&quot;\\n  input: &quot;gradient_descent/apply_gradient/learning_rate&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/hidden_layer_2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden_layer_2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/apply_gradient/update_output_layer/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;output_layer/kernel&quot;\\n  input: &quot;gradient_descent/apply_gradient/learning_rate&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/output_layer/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output_layer/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/apply_gradient/update_output_layer/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;output_layer/bias&quot;\\n  input: &quot;gradient_descent/apply_gradient/learning_rate&quot;\\n  input: &quot;gradient_descent/gradients/nn-architecture/output_layer/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@output_layer/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradient_descent/apply_gradient&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradient_descent/apply_gradient/update_hidden_layer_1/bias/ApplyGradientDescent&quot;\\n  input: &quot;^gradient_descent/apply_gradient/update_hidden_layer_1/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^gradient_descent/apply_gradient/update_hidden_layer_2/bias/ApplyGradientDescent&quot;\\n  input: &quot;^gradient_descent/apply_gradient/update_hidden_layer_2/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^gradient_descent/apply_gradient/update_output_layer/bias/ApplyGradientDescent&quot;\\n  input: &quot;^gradient_descent/apply_gradient/update_output_layer/kernel/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;validation/is_correct_label_vec/is_correct_label_vec/k&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;validation/is_correct_label_vec/is_correct_label_vec&quot;\\n  op: &quot;InTopKV2&quot;\\n  input: &quot;nn-architecture/output_layer/BiasAdd&quot;\\n  input: &quot;y&quot;\\n  input: &quot;validation/is_correct_label_vec/is_correct_label_vec/k&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;validation/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;validation/is_correct_label_vec/is_correct_label_vec&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;validation/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;validation/accuracy&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;validation/Cast&quot;\\n  input: &quot;validation/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.7599694552394674&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution phase\n",
    "Note that the only information of the dataset used so far is the number of labels and features. This is to remark that we don't actually need any data to construct the computation graph.\n",
    "\n",
    "We are now ready to run the computational graph providing mini-batches of 50 observations each. The main logic added here is related with saving the model and evaluation metrics periodically. We do not discuss design choices because we are mainly focusing on learning the tesorflow API as opposed to improve the validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "batch_iters = len(X_train) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Training accuracy: 0.9659761786460876; Validation accuracy: 0.9564999938011169.\n",
      "Epoch: 1; Training accuracy: 0.9764761924743652; Validation accuracy: 0.964722216129303.\n",
      "Epoch: 2; Training accuracy: 0.9784285426139832; Validation accuracy: 0.9628333449363708.\n",
      "Epoch: 3; Training accuracy: 0.986547589302063; Validation accuracy: 0.9695000052452087.\n",
      "Epoch: 4; Training accuracy: 0.9820952415466309; Validation accuracy: 0.9643333554267883.\n",
      "Epoch: 5; Training accuracy: 0.9880238175392151; Validation accuracy: 0.9712222218513489.\n",
      "Epoch: 6; Training accuracy: 0.9895952343940735; Validation accuracy: 0.9704444408416748.\n",
      "Epoch: 7; Training accuracy: 0.9887857437133789; Validation accuracy: 0.9684444665908813.\n",
      "Epoch: 8; Training accuracy: 0.9948333501815796; Validation accuracy: 0.9743888974189758.\n",
      "Epoch: 9; Training accuracy: 0.9937381148338318; Validation accuracy: 0.9721111059188843.\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "# Construction phase. Save summary statistics for Tensorboard.\n",
    "dir_log = 'tf_logs/run_{}/'.format(datetime.utcnow().strftime('%Y%m%d_%H%M%S'))\n",
    "summary_accuracy_train = tf.summary.scalar('Accuracy_train', accuracy)\n",
    "summary_accuracy_valid = tf.summary.scalar('Accuracy_valid', accuracy)\n",
    "file_writer = tf.summary.FileWriter(dir_log, graph=tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_iter in range(batch_iters):\n",
    "            batch_idx_start = batch_iter * batch_size\n",
    "            batch_idx_end = (1 + batch_iter) * batch_size\n",
    "            X_batch = X_train[batch_idx_start:batch_idx_end]\n",
    "            y_batch = y_train[batch_idx_start:batch_idx_end]\n",
    "            session.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_train = session.run(accuracy, feed_dict={X: X_train, y: y_train})\n",
    "        accuracy_valid = session.run(accuracy, feed_dict={X: X_valid, y: y_valid})\n",
    "        print('Epoch: {}; Training accuracy: {}; Validation accuracy: {}.'.format(epoch, accuracy_train, accuracy_valid))\n",
    "        \n",
    "        # Save model\n",
    "        path_model = os.path.join('tf_checkpoints', 'session.ckpt')\n",
    "        saver.save(session, path_model)\n",
    "        \n",
    "        # Save summary stats.\n",
    "        file_writer.add_summary(\n",
    "            summary=session.run(summary_accuracy_train, feed_dict={X: X_train, y: y_train}),\n",
    "            global_step=epoch,\n",
    "        )\n",
    "        file_writer.add_summary(\n",
    "            summary=session.run(summary_accuracy_valid, feed_dict={X: X_valid, y: y_valid}),\n",
    "            global_step=epoch,\n",
    "        )\n",
    "        # Flush. See: https://github.com/tensorflow/tensorflow/issues/2353#issuecomment-287516024\n",
    "        file_writer.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-trained model and compute test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tf_checkpoints/session.ckpt\n",
      "Test accuracy: 0.975600004196167\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    # Load pre-trained model and print test accuracy.\n",
    "    loader = tf.train.import_meta_graph(path_model + '.meta')\n",
    "    loader.restore(session, path_model)\n",
    "    accuracy_test = session.run(accuracy, feed_dict={X: X_test, y: y_test})\n",
    "    print('Test accuracy: {}'.format(accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some misslabelled test digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tf_checkpoints/session.ckpt\n"
     ]
    }
   ],
   "source": [
    "p_hat = tf.nn.softmax(logits)\n",
    "y_hat = tf.argmax(p_hat, axis=1)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    # Load pre-trained model and print test accuracy.\n",
    "    loader = tf.train.import_meta_graph(path_model + '.meta')\n",
    "    loader.restore(session, path_model)\n",
    "    p_hat_eval = session.run(p_hat, feed_dict={X: X_test, y: y_test})\n",
    "    y_hat_eval = session.run(y_hat, feed_dict={X: X_test, y: y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_hat</th>\n",
       "      <th>y</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>p_hat_gap</th>\n",
       "      <th>p_hat_highest</th>\n",
       "      <th>p_hat_true</th>\n",
       "      <th>p_hat(y=0)</th>\n",
       "      <th>p_hat(y=1)</th>\n",
       "      <th>p_hat(y=2)</th>\n",
       "      <th>p_hat(y=3)</th>\n",
       "      <th>p_hat(y=4)</th>\n",
       "      <th>p_hat(y=5)</th>\n",
       "      <th>p_hat(y=6)</th>\n",
       "      <th>p_hat(y=7)</th>\n",
       "      <th>p_hat(y=8)</th>\n",
       "      <th>p_hat(y=9)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2654</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.799955e-10</td>\n",
       "      <td>6.039241e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.375087e-09</td>\n",
       "      <td>6.340134e-17</td>\n",
       "      <td>1.108508e-10</td>\n",
       "      <td>2.419757e-15</td>\n",
       "      <td>9.799955e-10</td>\n",
       "      <td>2.874633e-08</td>\n",
       "      <td>6.264926e-11</td>\n",
       "      <td>1.903757e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.508886e-08</td>\n",
       "      <td>1.719988e-09</td>\n",
       "      <td>9.999996e-01</td>\n",
       "      <td>2.663332e-11</td>\n",
       "      <td>4.633816e-10</td>\n",
       "      <td>2.004915e-08</td>\n",
       "      <td>1.925432e-09</td>\n",
       "      <td>6.508886e-08</td>\n",
       "      <td>2.965463e-07</td>\n",
       "      <td>3.212247e-08</td>\n",
       "      <td>7.028678e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9664</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>1.057209e-09</td>\n",
       "      <td>1.517011e-12</td>\n",
       "      <td>7.939868e-07</td>\n",
       "      <td>1.057209e-09</td>\n",
       "      <td>2.608684e-06</td>\n",
       "      <td>1.029905e-08</td>\n",
       "      <td>2.779755e-13</td>\n",
       "      <td>5.960672e-14</td>\n",
       "      <td>9.999923e-01</td>\n",
       "      <td>5.207698e-09</td>\n",
       "      <td>4.328211e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9729</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>6.419147e-07</td>\n",
       "      <td>2.068320e-05</td>\n",
       "      <td>2.590340e-08</td>\n",
       "      <td>7.568501e-09</td>\n",
       "      <td>1.236517e-08</td>\n",
       "      <td>1.798676e-08</td>\n",
       "      <td>6.419147e-07</td>\n",
       "      <td>9.999785e-01</td>\n",
       "      <td>5.038726e-10</td>\n",
       "      <td>2.902442e-09</td>\n",
       "      <td>1.491900e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5887</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999972</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>1.375149e-05</td>\n",
       "      <td>9.999856e-01</td>\n",
       "      <td>1.511929e-11</td>\n",
       "      <td>1.060454e-09</td>\n",
       "      <td>1.676376e-08</td>\n",
       "      <td>4.353042e-09</td>\n",
       "      <td>3.931498e-09</td>\n",
       "      <td>5.025538e-07</td>\n",
       "      <td>1.375149e-05</td>\n",
       "      <td>9.628124e-13</td>\n",
       "      <td>1.323623e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999918</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>3.685551e-05</td>\n",
       "      <td>8.829818e-09</td>\n",
       "      <td>6.424936e-06</td>\n",
       "      <td>9.999545e-01</td>\n",
       "      <td>1.473153e-07</td>\n",
       "      <td>4.881197e-11</td>\n",
       "      <td>8.623062e-11</td>\n",
       "      <td>1.920501e-08</td>\n",
       "      <td>3.685551e-05</td>\n",
       "      <td>2.152878e-06</td>\n",
       "      <td>3.423246e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999883</td>\n",
       "      <td>0.999938</td>\n",
       "      <td>5.499030e-05</td>\n",
       "      <td>8.297365e-09</td>\n",
       "      <td>3.761675e-07</td>\n",
       "      <td>2.654635e-08</td>\n",
       "      <td>9.999377e-01</td>\n",
       "      <td>3.001270e-06</td>\n",
       "      <td>5.499030e-05</td>\n",
       "      <td>4.455344e-09</td>\n",
       "      <td>1.261698e-08</td>\n",
       "      <td>2.625407e-06</td>\n",
       "      <td>1.364755e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999873</td>\n",
       "      <td>0.999873</td>\n",
       "      <td>5.446462e-08</td>\n",
       "      <td>1.381946e-09</td>\n",
       "      <td>4.159984e-08</td>\n",
       "      <td>7.754627e-11</td>\n",
       "      <td>3.454816e-13</td>\n",
       "      <td>1.862113e-09</td>\n",
       "      <td>5.446462e-08</td>\n",
       "      <td>1.264931e-04</td>\n",
       "      <td>1.926084e-10</td>\n",
       "      <td>9.998734e-01</td>\n",
       "      <td>1.261698e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999873</td>\n",
       "      <td>0.999936</td>\n",
       "      <td>6.325457e-05</td>\n",
       "      <td>9.808149e-12</td>\n",
       "      <td>6.476016e-09</td>\n",
       "      <td>2.072887e-07</td>\n",
       "      <td>6.325457e-05</td>\n",
       "      <td>2.643282e-13</td>\n",
       "      <td>1.800418e-13</td>\n",
       "      <td>6.549169e-12</td>\n",
       "      <td>9.999365e-01</td>\n",
       "      <td>1.556421e-09</td>\n",
       "      <td>5.117501e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999793</td>\n",
       "      <td>0.999808</td>\n",
       "      <td>1.467678e-05</td>\n",
       "      <td>6.166593e-09</td>\n",
       "      <td>1.722771e-04</td>\n",
       "      <td>1.528059e-07</td>\n",
       "      <td>3.728054e-06</td>\n",
       "      <td>4.401842e-08</td>\n",
       "      <td>5.618529e-09</td>\n",
       "      <td>1.450175e-09</td>\n",
       "      <td>9.998075e-01</td>\n",
       "      <td>1.467678e-05</td>\n",
       "      <td>1.614938e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999745</td>\n",
       "      <td>0.999748</td>\n",
       "      <td>2.586339e-06</td>\n",
       "      <td>1.196091e-07</td>\n",
       "      <td>9.997481e-01</td>\n",
       "      <td>2.151977e-09</td>\n",
       "      <td>1.487258e-08</td>\n",
       "      <td>1.977010e-07</td>\n",
       "      <td>1.411247e-05</td>\n",
       "      <td>2.586339e-06</td>\n",
       "      <td>1.559946e-04</td>\n",
       "      <td>7.898557e-05</td>\n",
       "      <td>8.006709e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999700</td>\n",
       "      <td>0.999839</td>\n",
       "      <td>1.388276e-04</td>\n",
       "      <td>9.998387e-01</td>\n",
       "      <td>2.240470e-14</td>\n",
       "      <td>8.215247e-12</td>\n",
       "      <td>1.206786e-11</td>\n",
       "      <td>6.623529e-14</td>\n",
       "      <td>1.946740e-10</td>\n",
       "      <td>1.388276e-04</td>\n",
       "      <td>2.237545e-05</td>\n",
       "      <td>5.006916e-14</td>\n",
       "      <td>1.049167e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4315</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999673</td>\n",
       "      <td>0.999769</td>\n",
       "      <td>9.629534e-05</td>\n",
       "      <td>4.338732e-09</td>\n",
       "      <td>2.995166e-07</td>\n",
       "      <td>1.297397e-10</td>\n",
       "      <td>4.635132e-08</td>\n",
       "      <td>1.968923e-06</td>\n",
       "      <td>9.629534e-05</td>\n",
       "      <td>6.632631e-05</td>\n",
       "      <td>2.261422e-09</td>\n",
       "      <td>9.997694e-01</td>\n",
       "      <td>6.561101e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3520</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999659</td>\n",
       "      <td>0.999659</td>\n",
       "      <td>1.204761e-11</td>\n",
       "      <td>1.270200e-10</td>\n",
       "      <td>7.908135e-12</td>\n",
       "      <td>4.516492e-13</td>\n",
       "      <td>2.925810e-13</td>\n",
       "      <td>9.996589e-01</td>\n",
       "      <td>2.720714e-16</td>\n",
       "      <td>1.204761e-11</td>\n",
       "      <td>3.410823e-04</td>\n",
       "      <td>1.932566e-14</td>\n",
       "      <td>1.076971e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999584</td>\n",
       "      <td>0.999687</td>\n",
       "      <td>1.030463e-04</td>\n",
       "      <td>9.990108e-08</td>\n",
       "      <td>1.570521e-06</td>\n",
       "      <td>5.101501e-09</td>\n",
       "      <td>6.869360e-05</td>\n",
       "      <td>6.455684e-05</td>\n",
       "      <td>1.030463e-04</td>\n",
       "      <td>1.318298e-09</td>\n",
       "      <td>4.992649e-05</td>\n",
       "      <td>2.494146e-05</td>\n",
       "      <td>9.996872e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9613</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999553</td>\n",
       "      <td>0.999571</td>\n",
       "      <td>1.724511e-05</td>\n",
       "      <td>2.200055e-11</td>\n",
       "      <td>9.995705e-01</td>\n",
       "      <td>1.724511e-05</td>\n",
       "      <td>1.065562e-04</td>\n",
       "      <td>8.641069e-11</td>\n",
       "      <td>1.861182e-11</td>\n",
       "      <td>5.414378e-11</td>\n",
       "      <td>3.037859e-04</td>\n",
       "      <td>1.761619e-06</td>\n",
       "      <td>2.480354e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3503</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999404</td>\n",
       "      <td>0.999432</td>\n",
       "      <td>2.844659e-05</td>\n",
       "      <td>6.259105e-09</td>\n",
       "      <td>9.994320e-01</td>\n",
       "      <td>3.947518e-06</td>\n",
       "      <td>6.904022e-06</td>\n",
       "      <td>4.741675e-05</td>\n",
       "      <td>1.066342e-06</td>\n",
       "      <td>5.412833e-07</td>\n",
       "      <td>2.798483e-04</td>\n",
       "      <td>1.997091e-04</td>\n",
       "      <td>2.844659e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9770</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999006</td>\n",
       "      <td>0.999040</td>\n",
       "      <td>3.375815e-05</td>\n",
       "      <td>9.990398e-01</td>\n",
       "      <td>5.729918e-08</td>\n",
       "      <td>8.884062e-06</td>\n",
       "      <td>2.449048e-07</td>\n",
       "      <td>1.586946e-06</td>\n",
       "      <td>3.375815e-05</td>\n",
       "      <td>9.123310e-04</td>\n",
       "      <td>3.198113e-06</td>\n",
       "      <td>1.664088e-09</td>\n",
       "      <td>1.306859e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>0.998789</td>\n",
       "      <td>0.998941</td>\n",
       "      <td>1.512335e-04</td>\n",
       "      <td>1.199620e-08</td>\n",
       "      <td>9.989405e-01</td>\n",
       "      <td>1.639759e-05</td>\n",
       "      <td>5.362183e-08</td>\n",
       "      <td>5.798148e-04</td>\n",
       "      <td>8.435801e-08</td>\n",
       "      <td>6.888676e-07</td>\n",
       "      <td>3.061323e-04</td>\n",
       "      <td>4.952889e-06</td>\n",
       "      <td>1.512335e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3405</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.998671</td>\n",
       "      <td>0.999317</td>\n",
       "      <td>6.457570e-04</td>\n",
       "      <td>8.129080e-14</td>\n",
       "      <td>3.917599e-10</td>\n",
       "      <td>1.170088e-13</td>\n",
       "      <td>1.261999e-10</td>\n",
       "      <td>6.457570e-04</td>\n",
       "      <td>3.307140e-13</td>\n",
       "      <td>1.077420e-12</td>\n",
       "      <td>3.703148e-05</td>\n",
       "      <td>2.479453e-13</td>\n",
       "      <td>9.993172e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2109</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.998252</td>\n",
       "      <td>0.998257</td>\n",
       "      <td>5.203936e-06</td>\n",
       "      <td>3.424225e-08</td>\n",
       "      <td>7.338853e-04</td>\n",
       "      <td>4.561845e-08</td>\n",
       "      <td>5.203936e-06</td>\n",
       "      <td>6.960788e-04</td>\n",
       "      <td>3.173336e-06</td>\n",
       "      <td>5.356673e-08</td>\n",
       "      <td>9.982567e-01</td>\n",
       "      <td>1.790009e-05</td>\n",
       "      <td>2.868616e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.996807</td>\n",
       "      <td>0.996810</td>\n",
       "      <td>2.226792e-06</td>\n",
       "      <td>4.929760e-06</td>\n",
       "      <td>8.432347e-04</td>\n",
       "      <td>2.226792e-06</td>\n",
       "      <td>3.632217e-07</td>\n",
       "      <td>2.833663e-07</td>\n",
       "      <td>1.992686e-05</td>\n",
       "      <td>6.202179e-04</td>\n",
       "      <td>9.968097e-01</td>\n",
       "      <td>1.698663e-03</td>\n",
       "      <td>4.882572e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8094</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.996513</td>\n",
       "      <td>0.996999</td>\n",
       "      <td>4.860931e-04</td>\n",
       "      <td>2.750194e-05</td>\n",
       "      <td>1.332626e-05</td>\n",
       "      <td>4.860931e-04</td>\n",
       "      <td>2.210855e-03</td>\n",
       "      <td>3.187145e-07</td>\n",
       "      <td>1.594256e-07</td>\n",
       "      <td>1.393267e-06</td>\n",
       "      <td>2.437874e-06</td>\n",
       "      <td>9.969994e-01</td>\n",
       "      <td>2.585171e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9742</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.995580</td>\n",
       "      <td>0.997784</td>\n",
       "      <td>2.203834e-03</td>\n",
       "      <td>1.986992e-08</td>\n",
       "      <td>1.096957e-05</td>\n",
       "      <td>1.238422e-06</td>\n",
       "      <td>2.203834e-03</td>\n",
       "      <td>8.585791e-10</td>\n",
       "      <td>4.315466e-08</td>\n",
       "      <td>7.261049e-08</td>\n",
       "      <td>1.127669e-09</td>\n",
       "      <td>9.977838e-01</td>\n",
       "      <td>1.001410e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>0.995120</td>\n",
       "      <td>0.997315</td>\n",
       "      <td>2.195085e-03</td>\n",
       "      <td>3.135159e-08</td>\n",
       "      <td>1.818898e-06</td>\n",
       "      <td>2.198684e-07</td>\n",
       "      <td>9.065414e-08</td>\n",
       "      <td>1.437743e-07</td>\n",
       "      <td>4.626102e-04</td>\n",
       "      <td>2.195085e-03</td>\n",
       "      <td>8.204788e-08</td>\n",
       "      <td>9.973151e-01</td>\n",
       "      <td>2.482992e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4384</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.993748</td>\n",
       "      <td>0.996870</td>\n",
       "      <td>3.121421e-03</td>\n",
       "      <td>5.690267e-10</td>\n",
       "      <td>9.968695e-01</td>\n",
       "      <td>3.121421e-03</td>\n",
       "      <td>7.970837e-06</td>\n",
       "      <td>5.728599e-10</td>\n",
       "      <td>2.679220e-11</td>\n",
       "      <td>3.406444e-10</td>\n",
       "      <td>3.403942e-07</td>\n",
       "      <td>7.260143e-07</td>\n",
       "      <td>1.375713e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4248</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.993219</td>\n",
       "      <td>0.994112</td>\n",
       "      <td>8.933824e-04</td>\n",
       "      <td>9.059017e-04</td>\n",
       "      <td>9.488717e-04</td>\n",
       "      <td>8.933824e-04</td>\n",
       "      <td>7.815089e-05</td>\n",
       "      <td>1.591237e-04</td>\n",
       "      <td>4.136085e-05</td>\n",
       "      <td>2.060181e-03</td>\n",
       "      <td>3.408853e-04</td>\n",
       "      <td>9.941123e-01</td>\n",
       "      <td>4.598195e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4571</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>0.991076</td>\n",
       "      <td>0.994245</td>\n",
       "      <td>3.169384e-03</td>\n",
       "      <td>9.942450e-01</td>\n",
       "      <td>6.430301e-07</td>\n",
       "      <td>2.543074e-03</td>\n",
       "      <td>1.442272e-05</td>\n",
       "      <td>6.426346e-09</td>\n",
       "      <td>1.194492e-06</td>\n",
       "      <td>3.169384e-03</td>\n",
       "      <td>1.123975e-05</td>\n",
       "      <td>1.033508e-05</td>\n",
       "      <td>4.730019e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5955</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.989909</td>\n",
       "      <td>0.994141</td>\n",
       "      <td>4.231801e-03</td>\n",
       "      <td>5.329864e-06</td>\n",
       "      <td>4.283143e-08</td>\n",
       "      <td>4.118525e-06</td>\n",
       "      <td>4.231801e-03</td>\n",
       "      <td>5.581181e-08</td>\n",
       "      <td>4.671048e-09</td>\n",
       "      <td>4.943727e-07</td>\n",
       "      <td>4.573248e-08</td>\n",
       "      <td>9.941410e-01</td>\n",
       "      <td>1.617120e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6662</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>0.989864</td>\n",
       "      <td>0.994431</td>\n",
       "      <td>4.567207e-03</td>\n",
       "      <td>1.247022e-05</td>\n",
       "      <td>1.156622e-07</td>\n",
       "      <td>9.944311e-01</td>\n",
       "      <td>9.825691e-04</td>\n",
       "      <td>3.360305e-09</td>\n",
       "      <td>9.249477e-09</td>\n",
       "      <td>3.750347e-07</td>\n",
       "      <td>4.567207e-03</td>\n",
       "      <td>4.995286e-06</td>\n",
       "      <td>1.200657e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3403</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>9.999992e-01</td>\n",
       "      <td>3.208829e-11</td>\n",
       "      <td>7.391376e-11</td>\n",
       "      <td>1.361778e-12</td>\n",
       "      <td>4.209638e-16</td>\n",
       "      <td>7.836300e-07</td>\n",
       "      <td>3.055463e-15</td>\n",
       "      <td>9.999992e-01</td>\n",
       "      <td>9.524467e-14</td>\n",
       "      <td>4.850585e-14</td>\n",
       "      <td>4.280950e-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3404</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>9.999994e-01</td>\n",
       "      <td>1.474189e-08</td>\n",
       "      <td>5.522319e-13</td>\n",
       "      <td>1.660466e-12</td>\n",
       "      <td>2.274742e-07</td>\n",
       "      <td>3.626731e-07</td>\n",
       "      <td>5.759176e-12</td>\n",
       "      <td>1.426364e-11</td>\n",
       "      <td>1.791759e-08</td>\n",
       "      <td>2.044461e-09</td>\n",
       "      <td>9.999994e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3406</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999921</td>\n",
       "      <td>9.999206e-01</td>\n",
       "      <td>4.279071e-08</td>\n",
       "      <td>2.026615e-06</td>\n",
       "      <td>1.887866e-08</td>\n",
       "      <td>5.109959e-05</td>\n",
       "      <td>1.130302e-08</td>\n",
       "      <td>1.174610e-05</td>\n",
       "      <td>5.145191e-07</td>\n",
       "      <td>2.163748e-07</td>\n",
       "      <td>9.999206e-01</td>\n",
       "      <td>1.376361e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3407</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.999996e-01</td>\n",
       "      <td>1.386809e-11</td>\n",
       "      <td>5.920640e-10</td>\n",
       "      <td>1.683025e-08</td>\n",
       "      <td>9.999996e-01</td>\n",
       "      <td>1.166707e-09</td>\n",
       "      <td>3.145475e-08</td>\n",
       "      <td>8.459325e-14</td>\n",
       "      <td>5.435531e-10</td>\n",
       "      <td>3.338928e-08</td>\n",
       "      <td>3.828623e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3408</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.901392e-20</td>\n",
       "      <td>5.810613e-14</td>\n",
       "      <td>1.833808e-17</td>\n",
       "      <td>9.139866e-09</td>\n",
       "      <td>1.293531e-19</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.471178e-17</td>\n",
       "      <td>5.125108e-22</td>\n",
       "      <td>1.236858e-14</td>\n",
       "      <td>3.573235e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3409</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.072037e-18</td>\n",
       "      <td>8.953066e-14</td>\n",
       "      <td>1.450700e-14</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.205675e-19</td>\n",
       "      <td>4.136166e-15</td>\n",
       "      <td>5.884333e-22</td>\n",
       "      <td>6.672012e-17</td>\n",
       "      <td>1.612026e-11</td>\n",
       "      <td>2.215063e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3410</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>9.999918e-01</td>\n",
       "      <td>6.929239e-12</td>\n",
       "      <td>8.155235e-09</td>\n",
       "      <td>8.635180e-16</td>\n",
       "      <td>2.026560e-12</td>\n",
       "      <td>9.999918e-01</td>\n",
       "      <td>3.318878e-14</td>\n",
       "      <td>4.059766e-13</td>\n",
       "      <td>5.949823e-06</td>\n",
       "      <td>8.685897e-15</td>\n",
       "      <td>2.224833e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3411</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999782</td>\n",
       "      <td>9.997820e-01</td>\n",
       "      <td>1.662300e-09</td>\n",
       "      <td>1.196082e-09</td>\n",
       "      <td>2.087564e-11</td>\n",
       "      <td>7.157037e-06</td>\n",
       "      <td>7.194229e-06</td>\n",
       "      <td>1.381264e-12</td>\n",
       "      <td>2.767788e-12</td>\n",
       "      <td>2.033767e-04</td>\n",
       "      <td>2.492021e-07</td>\n",
       "      <td>9.997820e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3412</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.999998e-01</td>\n",
       "      <td>9.999998e-01</td>\n",
       "      <td>1.571855e-12</td>\n",
       "      <td>3.904198e-08</td>\n",
       "      <td>7.404394e-10</td>\n",
       "      <td>4.476998e-11</td>\n",
       "      <td>1.449454e-12</td>\n",
       "      <td>1.460668e-09</td>\n",
       "      <td>1.064121e-07</td>\n",
       "      <td>2.981301e-10</td>\n",
       "      <td>1.591468e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3413</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>9.999987e-01</td>\n",
       "      <td>9.999987e-01</td>\n",
       "      <td>2.732411e-12</td>\n",
       "      <td>7.988151e-08</td>\n",
       "      <td>5.025074e-14</td>\n",
       "      <td>8.768528e-13</td>\n",
       "      <td>6.126909e-13</td>\n",
       "      <td>1.233076e-06</td>\n",
       "      <td>3.140706e-12</td>\n",
       "      <td>2.633745e-12</td>\n",
       "      <td>2.183334e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3397</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.537834e-18</td>\n",
       "      <td>5.138828e-16</td>\n",
       "      <td>6.004621e-24</td>\n",
       "      <td>2.713778e-20</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.159653e-22</td>\n",
       "      <td>2.388615e-20</td>\n",
       "      <td>2.170483e-12</td>\n",
       "      <td>6.893139e-23</td>\n",
       "      <td>2.363694e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3395</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>1.809982e-08</td>\n",
       "      <td>1.041819e-11</td>\n",
       "      <td>7.260260e-12</td>\n",
       "      <td>2.771014e-10</td>\n",
       "      <td>6.532007e-08</td>\n",
       "      <td>3.439716e-09</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>3.596352e-12</td>\n",
       "      <td>4.488702e-08</td>\n",
       "      <td>2.240041e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3378</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>9.999964e-01</td>\n",
       "      <td>1.021666e-09</td>\n",
       "      <td>5.043395e-09</td>\n",
       "      <td>2.401151e-11</td>\n",
       "      <td>3.877365e-08</td>\n",
       "      <td>3.010863e-11</td>\n",
       "      <td>5.157051e-13</td>\n",
       "      <td>4.507671e-13</td>\n",
       "      <td>9.999964e-01</td>\n",
       "      <td>1.737622e-10</td>\n",
       "      <td>3.535658e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3394</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999905</td>\n",
       "      <td>9.999051e-01</td>\n",
       "      <td>5.445809e-10</td>\n",
       "      <td>5.741400e-13</td>\n",
       "      <td>4.365387e-13</td>\n",
       "      <td>3.662292e-10</td>\n",
       "      <td>6.585519e-07</td>\n",
       "      <td>1.115770e-13</td>\n",
       "      <td>7.163656e-13</td>\n",
       "      <td>9.420474e-05</td>\n",
       "      <td>7.013557e-13</td>\n",
       "      <td>9.999051e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3379</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>9.999992e-01</td>\n",
       "      <td>4.244099e-14</td>\n",
       "      <td>9.999992e-01</td>\n",
       "      <td>6.759530e-10</td>\n",
       "      <td>2.989506e-13</td>\n",
       "      <td>1.624410e-08</td>\n",
       "      <td>3.720701e-12</td>\n",
       "      <td>4.853835e-10</td>\n",
       "      <td>7.972345e-07</td>\n",
       "      <td>6.116705e-10</td>\n",
       "      <td>3.794978e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3380</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>9.999906e-01</td>\n",
       "      <td>1.276322e-11</td>\n",
       "      <td>9.999906e-01</td>\n",
       "      <td>9.014327e-10</td>\n",
       "      <td>3.162961e-11</td>\n",
       "      <td>8.774487e-08</td>\n",
       "      <td>5.843167e-10</td>\n",
       "      <td>3.537558e-09</td>\n",
       "      <td>9.305229e-06</td>\n",
       "      <td>2.137363e-08</td>\n",
       "      <td>3.099949e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3381</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>8.369096e-15</td>\n",
       "      <td>1.878887e-12</td>\n",
       "      <td>3.879265e-12</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>2.006055e-16</td>\n",
       "      <td>2.869893e-16</td>\n",
       "      <td>7.441975e-17</td>\n",
       "      <td>2.690777e-11</td>\n",
       "      <td>1.762378e-07</td>\n",
       "      <td>1.808248e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3382</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.778319e-11</td>\n",
       "      <td>1.299002e-12</td>\n",
       "      <td>3.739296e-15</td>\n",
       "      <td>7.069212e-16</td>\n",
       "      <td>4.953082e-11</td>\n",
       "      <td>1.057089e-11</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.360137e-14</td>\n",
       "      <td>2.280470e-10</td>\n",
       "      <td>3.697791e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3383</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999945</td>\n",
       "      <td>9.999448e-01</td>\n",
       "      <td>7.784580e-08</td>\n",
       "      <td>1.480113e-08</td>\n",
       "      <td>2.102674e-11</td>\n",
       "      <td>1.110231e-13</td>\n",
       "      <td>4.822041e-10</td>\n",
       "      <td>5.208008e-09</td>\n",
       "      <td>9.999448e-01</td>\n",
       "      <td>6.581740e-10</td>\n",
       "      <td>5.501466e-05</td>\n",
       "      <td>1.219420e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3384</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.543942</td>\n",
       "      <td>5.439416e-01</td>\n",
       "      <td>3.812150e-01</td>\n",
       "      <td>9.507354e-04</td>\n",
       "      <td>5.439416e-01</td>\n",
       "      <td>1.144997e-05</td>\n",
       "      <td>4.695293e-07</td>\n",
       "      <td>2.600875e-05</td>\n",
       "      <td>7.375851e-02</td>\n",
       "      <td>9.613815e-05</td>\n",
       "      <td>6.185562e-08</td>\n",
       "      <td>5.440349e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3385</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>3.296429e-13</td>\n",
       "      <td>1.719476e-14</td>\n",
       "      <td>1.610463e-13</td>\n",
       "      <td>2.073038e-08</td>\n",
       "      <td>1.050895e-08</td>\n",
       "      <td>7.621363e-13</td>\n",
       "      <td>1.405112e-17</td>\n",
       "      <td>5.648551e-08</td>\n",
       "      <td>8.790899e-12</td>\n",
       "      <td>9.999999e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3386</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999981</td>\n",
       "      <td>9.999810e-01</td>\n",
       "      <td>1.934317e-11</td>\n",
       "      <td>9.999810e-01</td>\n",
       "      <td>2.722780e-12</td>\n",
       "      <td>2.836289e-10</td>\n",
       "      <td>1.584611e-07</td>\n",
       "      <td>1.661547e-09</td>\n",
       "      <td>2.080324e-10</td>\n",
       "      <td>1.869293e-05</td>\n",
       "      <td>2.365390e-08</td>\n",
       "      <td>8.634052e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3387</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.999995e-01</td>\n",
       "      <td>9.589642e-13</td>\n",
       "      <td>2.922309e-11</td>\n",
       "      <td>6.048550e-13</td>\n",
       "      <td>2.274109e-07</td>\n",
       "      <td>1.306641e-07</td>\n",
       "      <td>1.796670e-11</td>\n",
       "      <td>2.716320e-16</td>\n",
       "      <td>1.263229e-07</td>\n",
       "      <td>9.414743e-11</td>\n",
       "      <td>9.999995e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3388</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.963348</td>\n",
       "      <td>9.633479e-01</td>\n",
       "      <td>2.038859e-11</td>\n",
       "      <td>8.368257e-09</td>\n",
       "      <td>9.725296e-11</td>\n",
       "      <td>3.272419e-11</td>\n",
       "      <td>9.633479e-01</td>\n",
       "      <td>1.222298e-11</td>\n",
       "      <td>8.261863e-11</td>\n",
       "      <td>1.756646e-06</td>\n",
       "      <td>5.135414e-13</td>\n",
       "      <td>3.665039e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3389</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.917452e-12</td>\n",
       "      <td>4.102022e-11</td>\n",
       "      <td>1.398119e-10</td>\n",
       "      <td>4.426216e-11</td>\n",
       "      <td>2.634428e-15</td>\n",
       "      <td>6.550888e-15</td>\n",
       "      <td>4.649457e-12</td>\n",
       "      <td>2.420552e-14</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>7.244965e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3390</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.944418e-16</td>\n",
       "      <td>1.719760e-11</td>\n",
       "      <td>1.336098e-11</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.153421e-11</td>\n",
       "      <td>1.878707e-09</td>\n",
       "      <td>8.440615e-18</td>\n",
       "      <td>2.589065e-13</td>\n",
       "      <td>1.368544e-11</td>\n",
       "      <td>3.903788e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3391</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>8.768883e-11</td>\n",
       "      <td>1.333300e-11</td>\n",
       "      <td>2.001571e-13</td>\n",
       "      <td>2.083083e-14</td>\n",
       "      <td>2.293742e-08</td>\n",
       "      <td>1.343639e-11</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.738173e-13</td>\n",
       "      <td>3.744299e-11</td>\n",
       "      <td>2.218323e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3392</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.289473e-16</td>\n",
       "      <td>2.562212e-14</td>\n",
       "      <td>5.925350e-15</td>\n",
       "      <td>5.931479e-11</td>\n",
       "      <td>2.745193e-08</td>\n",
       "      <td>3.786798e-10</td>\n",
       "      <td>5.761272e-14</td>\n",
       "      <td>2.019612e-12</td>\n",
       "      <td>4.129317e-11</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3393</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>9.999940e-01</td>\n",
       "      <td>9.299425e-13</td>\n",
       "      <td>7.751448e-08</td>\n",
       "      <td>7.704593e-11</td>\n",
       "      <td>5.753015e-06</td>\n",
       "      <td>1.147306e-11</td>\n",
       "      <td>9.999940e-01</td>\n",
       "      <td>7.273032e-10</td>\n",
       "      <td>1.530009e-12</td>\n",
       "      <td>6.312180e-08</td>\n",
       "      <td>6.440459e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.128288e-10</td>\n",
       "      <td>7.855262e-12</td>\n",
       "      <td>1.170601e-13</td>\n",
       "      <td>4.605280e-13</td>\n",
       "      <td>1.134400e-08</td>\n",
       "      <td>2.886951e-09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.647809e-14</td>\n",
       "      <td>7.081727e-11</td>\n",
       "      <td>2.019712e-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows  16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      y_hat  y  is_correct  p_hat_gap  p_hat_highest    p_hat_true  \\\n",
       "2654      1  6       False   1.000000       1.000000  9.799955e-10   \n",
       "1181      1  6       False   1.000000       1.000000  6.508886e-08   \n",
       "9664      7  2       False   0.999992       0.999992  1.057209e-09   \n",
       "9729      6  5       False   0.999978       0.999979  6.419147e-07   \n",
       "5887      0  7       False   0.999972       0.999986  1.375149e-05   \n",
       "1226      2  7       False   0.999918       0.999954  3.685551e-05   \n",
       "2035      3  5       False   0.999883       0.999938  5.499030e-05   \n",
       "720       8  5       False   0.999873       0.999873  5.446462e-08   \n",
       "1681      7  3       False   0.999873       0.999936  6.325457e-05   \n",
       "4497      7  8       False   0.999793       0.999808  1.467678e-05   \n",
       "2135      1  6       False   0.999745       0.999748  2.586339e-06   \n",
       "445       0  6       False   0.999700       0.999839  1.388276e-04   \n",
       "4315      8  5       False   0.999673       0.999769  9.629534e-05   \n",
       "3520      4  6       False   0.999659       0.999659  1.204761e-11   \n",
       "1289      9  5       False   0.999584       0.999687  1.030463e-04   \n",
       "9613      1  2       False   0.999553       0.999571  1.724511e-05   \n",
       "3503      1  9       False   0.999404       0.999432  2.844659e-05   \n",
       "9770      0  5       False   0.999006       0.999040  3.375815e-05   \n",
       "2387      1  9       False   0.998789       0.998941  1.512335e-04   \n",
       "3405      9  4       False   0.998671       0.999317  6.457570e-04   \n",
       "2109      7  3       False   0.998252       0.998257  5.203936e-06   \n",
       "4176      7  2       False   0.996807       0.996810  2.226792e-06   \n",
       "8094      8  2       False   0.996513       0.996999  4.860931e-04   \n",
       "9742      8  3       False   0.995580       0.997784  2.203834e-03   \n",
       "1182      8  6       False   0.995120       0.997315  2.195085e-03   \n",
       "4384      1  2       False   0.993748       0.996870  3.121421e-03   \n",
       "4248      8  2       False   0.993219       0.994112  8.933824e-04   \n",
       "4571      0  6       False   0.991076       0.994245  3.169384e-03   \n",
       "5955      8  3       False   0.989909       0.994141  4.231801e-03   \n",
       "6662      2  7       False   0.989864       0.994431  4.567207e-03   \n",
       "...     ... ..         ...        ...            ...           ...   \n",
       "3403      6  6        True   0.000000       0.999999  9.999992e-01   \n",
       "3404      9  9        True   0.000000       0.999999  9.999994e-01   \n",
       "3406      8  8        True   0.000000       0.999921  9.999206e-01   \n",
       "3407      3  3        True   0.000000       1.000000  9.999996e-01   \n",
       "3408      5  5        True   0.000000       1.000000  1.000000e+00   \n",
       "3409      3  3        True   0.000000       1.000000  1.000000e+00   \n",
       "3410      4  4        True   0.000000       0.999992  9.999918e-01   \n",
       "3411      9  9        True   0.000000       0.999782  9.997820e-01   \n",
       "3412      0  0        True   0.000000       1.000000  9.999998e-01   \n",
       "3413      0  0        True   0.000000       0.999999  9.999987e-01   \n",
       "3397      4  4        True   0.000000       1.000000  1.000000e+00   \n",
       "3395      6  6        True   0.000000       1.000000  9.999999e-01   \n",
       "3378      7  7        True   0.000000       0.999996  9.999964e-01   \n",
       "3394      9  9        True   0.000000       0.999905  9.999051e-01   \n",
       "3379      1  1        True   0.000000       0.999999  9.999992e-01   \n",
       "3380      1  1        True   0.000000       0.999991  9.999906e-01   \n",
       "3381      3  3        True   0.000000       1.000000  9.999999e-01   \n",
       "3382      6  6        True   0.000000       1.000000  1.000000e+00   \n",
       "3383      6  6        True   0.000000       0.999945  9.999448e-01   \n",
       "3384      2  2        True   0.000000       0.543942  5.439416e-01   \n",
       "3385      9  9        True   0.000000       1.000000  9.999999e-01   \n",
       "3386      1  1        True   0.000000       0.999981  9.999810e-01   \n",
       "3387      9  9        True   0.000000       1.000000  9.999995e-01   \n",
       "3388      4  4        True   0.000000       0.963348  9.633479e-01   \n",
       "3389      8  8        True   0.000000       1.000000  1.000000e+00   \n",
       "3390      3  3        True   0.000000       1.000000  1.000000e+00   \n",
       "3391      6  6        True   0.000000       1.000000  1.000000e+00   \n",
       "3392      9  9        True   0.000000       1.000000  1.000000e+00   \n",
       "3393      5  5        True   0.000000       0.999994  9.999940e-01   \n",
       "9999      6  6        True   0.000000       1.000000  1.000000e+00   \n",
       "\n",
       "        p_hat(y=0)    p_hat(y=1)    p_hat(y=2)    p_hat(y=3)    p_hat(y=4)  \\\n",
       "2654  6.039241e-16  1.000000e+00  1.375087e-09  6.340134e-17  1.108508e-10   \n",
       "1181  1.719988e-09  9.999996e-01  2.663332e-11  4.633816e-10  2.004915e-08   \n",
       "9664  1.517011e-12  7.939868e-07  1.057209e-09  2.608684e-06  1.029905e-08   \n",
       "9729  2.068320e-05  2.590340e-08  7.568501e-09  1.236517e-08  1.798676e-08   \n",
       "5887  9.999856e-01  1.511929e-11  1.060454e-09  1.676376e-08  4.353042e-09   \n",
       "1226  8.829818e-09  6.424936e-06  9.999545e-01  1.473153e-07  4.881197e-11   \n",
       "2035  8.297365e-09  3.761675e-07  2.654635e-08  9.999377e-01  3.001270e-06   \n",
       "720   1.381946e-09  4.159984e-08  7.754627e-11  3.454816e-13  1.862113e-09   \n",
       "1681  9.808149e-12  6.476016e-09  2.072887e-07  6.325457e-05  2.643282e-13   \n",
       "4497  6.166593e-09  1.722771e-04  1.528059e-07  3.728054e-06  4.401842e-08   \n",
       "2135  1.196091e-07  9.997481e-01  2.151977e-09  1.487258e-08  1.977010e-07   \n",
       "445   9.998387e-01  2.240470e-14  8.215247e-12  1.206786e-11  6.623529e-14   \n",
       "4315  4.338732e-09  2.995166e-07  1.297397e-10  4.635132e-08  1.968923e-06   \n",
       "3520  1.270200e-10  7.908135e-12  4.516492e-13  2.925810e-13  9.996589e-01   \n",
       "1289  9.990108e-08  1.570521e-06  5.101501e-09  6.869360e-05  6.455684e-05   \n",
       "9613  2.200055e-11  9.995705e-01  1.724511e-05  1.065562e-04  8.641069e-11   \n",
       "3503  6.259105e-09  9.994320e-01  3.947518e-06  6.904022e-06  4.741675e-05   \n",
       "9770  9.990398e-01  5.729918e-08  8.884062e-06  2.449048e-07  1.586946e-06   \n",
       "2387  1.199620e-08  9.989405e-01  1.639759e-05  5.362183e-08  5.798148e-04   \n",
       "3405  8.129080e-14  3.917599e-10  1.170088e-13  1.261999e-10  6.457570e-04   \n",
       "2109  3.424225e-08  7.338853e-04  4.561845e-08  5.203936e-06  6.960788e-04   \n",
       "4176  4.929760e-06  8.432347e-04  2.226792e-06  3.632217e-07  2.833663e-07   \n",
       "8094  2.750194e-05  1.332626e-05  4.860931e-04  2.210855e-03  3.187145e-07   \n",
       "9742  1.986992e-08  1.096957e-05  1.238422e-06  2.203834e-03  8.585791e-10   \n",
       "1182  3.135159e-08  1.818898e-06  2.198684e-07  9.065414e-08  1.437743e-07   \n",
       "4384  5.690267e-10  9.968695e-01  3.121421e-03  7.970837e-06  5.728599e-10   \n",
       "4248  9.059017e-04  9.488717e-04  8.933824e-04  7.815089e-05  1.591237e-04   \n",
       "4571  9.942450e-01  6.430301e-07  2.543074e-03  1.442272e-05  6.426346e-09   \n",
       "5955  5.329864e-06  4.283143e-08  4.118525e-06  4.231801e-03  5.581181e-08   \n",
       "6662  1.247022e-05  1.156622e-07  9.944311e-01  9.825691e-04  3.360305e-09   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "3403  3.208829e-11  7.391376e-11  1.361778e-12  4.209638e-16  7.836300e-07   \n",
       "3404  1.474189e-08  5.522319e-13  1.660466e-12  2.274742e-07  3.626731e-07   \n",
       "3406  4.279071e-08  2.026615e-06  1.887866e-08  5.109959e-05  1.130302e-08   \n",
       "3407  1.386809e-11  5.920640e-10  1.683025e-08  9.999996e-01  1.166707e-09   \n",
       "3408  2.901392e-20  5.810613e-14  1.833808e-17  9.139866e-09  1.293531e-19   \n",
       "3409  6.072037e-18  8.953066e-14  1.450700e-14  1.000000e+00  3.205675e-19   \n",
       "3410  6.929239e-12  8.155235e-09  8.635180e-16  2.026560e-12  9.999918e-01   \n",
       "3411  1.662300e-09  1.196082e-09  2.087564e-11  7.157037e-06  7.194229e-06   \n",
       "3412  9.999998e-01  1.571855e-12  3.904198e-08  7.404394e-10  4.476998e-11   \n",
       "3413  9.999987e-01  2.732411e-12  7.988151e-08  5.025074e-14  8.768528e-13   \n",
       "3397  5.537834e-18  5.138828e-16  6.004621e-24  2.713778e-20  1.000000e+00   \n",
       "3395  1.809982e-08  1.041819e-11  7.260260e-12  2.771014e-10  6.532007e-08   \n",
       "3378  1.021666e-09  5.043395e-09  2.401151e-11  3.877365e-08  3.010863e-11   \n",
       "3394  5.445809e-10  5.741400e-13  4.365387e-13  3.662292e-10  6.585519e-07   \n",
       "3379  4.244099e-14  9.999992e-01  6.759530e-10  2.989506e-13  1.624410e-08   \n",
       "3380  1.276322e-11  9.999906e-01  9.014327e-10  3.162961e-11  8.774487e-08   \n",
       "3381  8.369096e-15  1.878887e-12  3.879265e-12  9.999999e-01  2.006055e-16   \n",
       "3382  3.778319e-11  1.299002e-12  3.739296e-15  7.069212e-16  4.953082e-11   \n",
       "3383  7.784580e-08  1.480113e-08  2.102674e-11  1.110231e-13  4.822041e-10   \n",
       "3384  3.812150e-01  9.507354e-04  5.439416e-01  1.144997e-05  4.695293e-07   \n",
       "3385  3.296429e-13  1.719476e-14  1.610463e-13  2.073038e-08  1.050895e-08   \n",
       "3386  1.934317e-11  9.999810e-01  2.722780e-12  2.836289e-10  1.584611e-07   \n",
       "3387  9.589642e-13  2.922309e-11  6.048550e-13  2.274109e-07  1.306641e-07   \n",
       "3388  2.038859e-11  8.368257e-09  9.725296e-11  3.272419e-11  9.633479e-01   \n",
       "3389  5.917452e-12  4.102022e-11  1.398119e-10  4.426216e-11  2.634428e-15   \n",
       "3390  6.944418e-16  1.719760e-11  1.336098e-11  1.000000e+00  1.153421e-11   \n",
       "3391  8.768883e-11  1.333300e-11  2.001571e-13  2.083083e-14  2.293742e-08   \n",
       "3392  1.289473e-16  2.562212e-14  5.925350e-15  5.931479e-11  2.745193e-08   \n",
       "3393  9.299425e-13  7.751448e-08  7.704593e-11  5.753015e-06  1.147306e-11   \n",
       "9999  1.128288e-10  7.855262e-12  1.170601e-13  4.605280e-13  1.134400e-08   \n",
       "\n",
       "        p_hat(y=5)    p_hat(y=6)    p_hat(y=7)    p_hat(y=8)    p_hat(y=9)  \n",
       "2654  2.419757e-15  9.799955e-10  2.874633e-08  6.264926e-11  1.903757e-15  \n",
       "1181  1.925432e-09  6.508886e-08  2.965463e-07  3.212247e-08  7.028678e-11  \n",
       "9664  2.779755e-13  5.960672e-14  9.999923e-01  5.207698e-09  4.328211e-06  \n",
       "9729  6.419147e-07  9.999785e-01  5.038726e-10  2.902442e-09  1.491900e-11  \n",
       "5887  3.931498e-09  5.025538e-07  1.375149e-05  9.628124e-13  1.323623e-07  \n",
       "1226  8.623062e-11  1.920501e-08  3.685551e-05  2.152878e-06  3.423246e-11  \n",
       "2035  5.499030e-05  4.455344e-09  1.261698e-08  2.625407e-06  1.364755e-06  \n",
       "720   5.446462e-08  1.264931e-04  1.926084e-10  9.998734e-01  1.261698e-10  \n",
       "1681  1.800418e-13  6.549169e-12  9.999365e-01  1.556421e-09  5.117501e-09  \n",
       "4497  5.618529e-09  1.450175e-09  9.998075e-01  1.467678e-05  1.614938e-06  \n",
       "2135  1.411247e-05  2.586339e-06  1.559946e-04  7.898557e-05  8.006709e-08  \n",
       "445   1.946740e-10  1.388276e-04  2.237545e-05  5.006916e-14  1.049167e-12  \n",
       "4315  9.629534e-05  6.632631e-05  2.261422e-09  9.997694e-01  6.561101e-05  \n",
       "3520  2.720714e-16  1.204761e-11  3.410823e-04  1.932566e-14  1.076971e-08  \n",
       "1289  1.030463e-04  1.318298e-09  4.992649e-05  2.494146e-05  9.996872e-01  \n",
       "9613  1.861182e-11  5.414378e-11  3.037859e-04  1.761619e-06  2.480354e-11  \n",
       "3503  1.066342e-06  5.412833e-07  2.798483e-04  1.997091e-04  2.844659e-05  \n",
       "9770  3.375815e-05  9.123310e-04  3.198113e-06  1.664088e-09  1.306859e-07  \n",
       "2387  8.435801e-08  6.888676e-07  3.061323e-04  4.952889e-06  1.512335e-04  \n",
       "3405  3.307140e-13  1.077420e-12  3.703148e-05  2.479453e-13  9.993172e-01  \n",
       "2109  3.173336e-06  5.356673e-08  9.982567e-01  1.790009e-05  2.868616e-04  \n",
       "4176  1.992686e-05  6.202179e-04  9.968097e-01  1.698663e-03  4.882572e-07  \n",
       "8094  1.594256e-07  1.393267e-06  2.437874e-06  9.969994e-01  2.585171e-04  \n",
       "9742  4.315466e-08  7.261049e-08  1.127669e-09  9.977838e-01  1.001410e-08  \n",
       "1182  4.626102e-04  2.195085e-03  8.204788e-08  9.973151e-01  2.482992e-05  \n",
       "4384  2.679220e-11  3.406444e-10  3.403942e-07  7.260143e-07  1.375713e-11  \n",
       "4248  4.136085e-05  2.060181e-03  3.408853e-04  9.941123e-01  4.598195e-04  \n",
       "4571  1.194492e-06  3.169384e-03  1.123975e-05  1.033508e-05  4.730019e-06  \n",
       "5955  4.671048e-09  4.943727e-07  4.573248e-08  9.941410e-01  1.617120e-03  \n",
       "6662  9.249477e-09  3.750347e-07  4.567207e-03  4.995286e-06  1.200657e-06  \n",
       "...            ...           ...           ...           ...           ...  \n",
       "3403  3.055463e-15  9.999992e-01  9.524467e-14  4.850585e-14  4.280950e-19  \n",
       "3404  5.759176e-12  1.426364e-11  1.791759e-08  2.044461e-09  9.999994e-01  \n",
       "3406  1.174610e-05  5.145191e-07  2.163748e-07  9.999206e-01  1.376361e-05  \n",
       "3407  3.145475e-08  8.459325e-14  5.435531e-10  3.338928e-08  3.828623e-07  \n",
       "3408  1.000000e+00  6.471178e-17  5.125108e-22  1.236858e-14  3.573235e-12  \n",
       "3409  4.136166e-15  5.884333e-22  6.672012e-17  1.612026e-11  2.215063e-14  \n",
       "3410  3.318878e-14  4.059766e-13  5.949823e-06  8.685897e-15  2.224833e-06  \n",
       "3411  1.381264e-12  2.767788e-12  2.033767e-04  2.492021e-07  9.997820e-01  \n",
       "3412  1.449454e-12  1.460668e-09  1.064121e-07  2.981301e-10  1.591468e-07  \n",
       "3413  6.126909e-13  1.233076e-06  3.140706e-12  2.633745e-12  2.183334e-13  \n",
       "3397  2.159653e-22  2.388615e-20  2.170483e-12  6.893139e-23  2.363694e-15  \n",
       "3395  3.439716e-09  9.999999e-01  3.596352e-12  4.488702e-08  2.240041e-10  \n",
       "3378  5.157051e-13  4.507671e-13  9.999964e-01  1.737622e-10  3.535658e-06  \n",
       "3394  1.115770e-13  7.163656e-13  9.420474e-05  7.013557e-13  9.999051e-01  \n",
       "3379  3.720701e-12  4.853835e-10  7.972345e-07  6.116705e-10  3.794978e-11  \n",
       "3380  5.843167e-10  3.537558e-09  9.305229e-06  2.137363e-08  3.099949e-08  \n",
       "3381  2.869893e-16  7.441975e-17  2.690777e-11  1.762378e-07  1.808248e-13  \n",
       "3382  1.057089e-11  1.000000e+00  1.360137e-14  2.280470e-10  3.697791e-15  \n",
       "3383  5.208008e-09  9.999448e-01  6.581740e-10  5.501466e-05  1.219420e-12  \n",
       "3384  2.600875e-05  7.375851e-02  9.613815e-05  6.185562e-08  5.440349e-09  \n",
       "3385  7.621363e-13  1.405112e-17  5.648551e-08  8.790899e-12  9.999999e-01  \n",
       "3386  1.661547e-09  2.080324e-10  1.869293e-05  2.365390e-08  8.634052e-08  \n",
       "3387  1.796670e-11  2.716320e-16  1.263229e-07  9.414743e-11  9.999995e-01  \n",
       "3388  1.222298e-11  8.261863e-11  1.756646e-06  5.135414e-13  3.665039e-02  \n",
       "3389  6.550888e-15  4.649457e-12  2.420552e-14  1.000000e+00  7.244965e-11  \n",
       "3390  1.878707e-09  8.440615e-18  2.589065e-13  1.368544e-11  3.903788e-09  \n",
       "3391  1.343639e-11  1.000000e+00  4.738173e-13  3.744299e-11  2.218323e-14  \n",
       "3392  3.786798e-10  5.761272e-14  2.019612e-12  4.129317e-11  1.000000e+00  \n",
       "3393  9.999940e-01  7.273032e-10  1.530009e-12  6.312180e-08  6.440459e-08  \n",
       "9999  2.886951e-09  1.000000e+00  2.647809e-14  7.081727e-11  2.019712e-12  \n",
       "\n",
       "[10000 rows x 16 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_hat_df = pd.DataFrame(p_hat_eval, columns=['p_hat(y={})'.format(i) for i in range(nr_labels)])\n",
    "p_hat_highest = [p_hat_df.iloc[i, y_hat_eval[i]] for i in range(len(X_test))]\n",
    "p_hat_true = [p_hat_df.iloc[i, y_test[i]] for i in range(len(X_test))]\n",
    "is_correct = y_hat_eval == y_test\n",
    "p_hat_gap = pd.Series(p_hat_highest) - pd.Series(p_hat_true)\n",
    "results = pd.concat([\n",
    "    pd.Series(y_hat_eval).to_frame('y_hat'),\n",
    "    pd.Series(y_test).to_frame('y'),\n",
    "    pd.Series(is_correct).to_frame('is_correct'),\n",
    "    pd.Series(p_hat_gap).to_frame('p_hat_gap'),\n",
    "    pd.Series(p_hat_highest).to_frame('p_hat_highest'),\n",
    "    pd.Series(p_hat_true).to_frame('p_hat_true'),\n",
    "    p_hat_df,\n",
    "], axis='columns')\n",
    "results.sort_values('p_hat_gap', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig=plt.figure()\n",
    "columns = 3\n",
    "rows = 1\n",
    "iters = columns * rows + 1\n",
    "results_sorted = results.sort_values('p_hat_gap', ascending=False)\n",
    "for i in range(1, iters):\n",
    "    idx_digit = results_sorted.index[i]\n",
    "    pixels = X_test[idx_digit].reshape(pixels_rows, pixels_cols)\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.title('y={}; y_hat={}'.format(results.loc[idx_digit, 'y'], results.loc[idx_digit, 'y_hat']))\n",
    "    plt.imshow(pixels, cmap='gray')\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book-hands-on-ml",
   "language": "python",
   "name": "book-hands-on-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "187px",
    "width": "323px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
