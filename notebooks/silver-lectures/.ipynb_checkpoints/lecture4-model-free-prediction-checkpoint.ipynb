{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Monte-Carlo-learning\" data-toc-modified-id=\"Monte-Carlo-learning-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Monte Carlo learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#First-Visit-Monte-Carlo-Policy-Evaluation\" data-toc-modified-id=\"First-Visit-Monte-Carlo-Policy-Evaluation-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>First-Visit Monte-Carlo Policy Evaluation</a></span></li><li><span><a href=\"#Every-Visit-Monte-Carlo-Policy-Evaluation\" data-toc-modified-id=\"Every-Visit-Monte-Carlo-Policy-Evaluation-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Every-Visit Monte-Carlo Policy Evaluation</a></span></li></ul></li><li><span><a href=\"#Temporal-Difference-learning\" data-toc-modified-id=\"Temporal-Difference-learning-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Temporal Difference learning</a></span></li><li><span><a href=\"#TD(λ)\" data-toc-modified-id=\"TD(λ)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>TD(λ)</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import defaultdict\n",
    "from typing import Callable, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "import time\n",
    "import cufflinks\n",
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "env.render()\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In model-free prediction we solve the task of estimating the state-value or action-value look-up table without assuming any knowledge about the transition matrix $\\mathcal{S}$ and the rewards dynamics $\\mathcal{R}$ of the Markov Decision Process (MDP) $<\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\lambda>$. This makes the algorithms applicable in the real world, were we generally don't know the dynamics of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo (MC) learns from complete finite episodes (no bootstrapping) from the simplest possible idea: sample episodes and calculate the empirical mean of state-value or action-values. By definition $v_π(s) \\doteq \\mathbb{E}_π [G_t | S_t = s]$. MC replace the expectation with an average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_random(env: gym.Env, obs: Any):\n",
    "    return env.action_space.sample()\n",
    "\n",
    "\n",
    "def generate_trajectory(env: gym.Env, policy: Callable, seed: int=0):\n",
    "    states, actions, rewards = [], [], []\n",
    "    \n",
    "    env.seed(seed)\n",
    "    obs = env.reset()\n",
    "    states.append(obs)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = policy(env, obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        states.append(obs)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "    actions.append(None)\n",
    "    rewards.append(None)\n",
    "    return states, actions, rewards\n",
    "\n",
    "\n",
    "def monte_carlo_leaning(policy: Callable, n_trajectories: int=1000, is_first_visit_mc: bool=True, gamma: int=1, alpha: float=None):\n",
    "    \"\"\"Set alpha is the problem is not stationary (to slowly forget the past with EWMA), empirical mean if none.\"\"\"\n",
    "    N = defaultdict(int)\n",
    "    V = defaultdict(float)\n",
    "    V_history = dict()\n",
    "    for nr_episode in range(n_trajectories):\n",
    "        states, actions, rewards = generate_trajectory(env, policy, seed=nr_episode)\n",
    "        for t in range(len(states) - 1):  # -1 to avoid terminal state\n",
    "            state = states[t]\n",
    "            action = actions[t]\n",
    "            reward = rewards[t]\n",
    "            is_already_visited = state in states[:t]\n",
    "            if is_first_visit_mc and is_already_visited:\n",
    "                continue\n",
    "            N[state] += 1\n",
    "            coeff = 1 / N[state] if alpha is None else alpha\n",
    "            G_t = sum(r * gamma ** i for i, r in enumerate(rewards[t:-1]))\n",
    "            target = G_t\n",
    "            \n",
    "            V[state] = V[state] + coeff * (target - V[state])  # running mean\n",
    "                \n",
    "        # Update state-value look-up table.\n",
    "        V_history[nr_episode] = V.copy()\n",
    "\n",
    "    return pd.DataFrame(V_history).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First-Visit Monte-Carlo Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_history = monte_carlo_leaning(policy=policy_random, gamma=1, n_trajectories=100000, is_first_visit_mc=True)\n",
    "V_history.plot()\n",
    "print('State-value look-up table:')\n",
    "pd.DataFrame(V_history.iloc[-1].combine_first(pd.Series('', index=range(16))).values.reshape(4, 4)).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Every-Visit Monte-Carlo Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_history = monte_carlo_leaning(policy=policy_random, gamma=1, n_trajectories=100000, is_first_visit_mc=False)\n",
    "V_history.plot()\n",
    "print('State-value look-up table:')\n",
    "pd.DataFrame(V_history.iloc[-1].combine_first(pd.Series('', index=range(16))).values.reshape(4, 4)).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference learning\n",
    "TD learns from incomplete episodes, by bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_random(env: gym.Env, obs: Any):\n",
    "    return env.action_space.sample()\n",
    "\n",
    "\n",
    "def monte_carlo_leaning(policy: Callable, n_trajectories: int=1000, is_every_visit_mc: bool=True, gamma: int=1, alpha: float=None, lambda_: float=0):\n",
    "    \"\"\"Set alpha is the problem is not stationary (to slowly forget the past with EWMA), empirical mean if none.\"\"\"\n",
    "    N = defaultdict(int)\n",
    "    V = defaultdict(float)\n",
    "    V_history = dict()\n",
    "    for nr_episode in range(n_trajectories):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        states, actions, rewards = [obs], [None], [None]\n",
    "        while not done:\n",
    "            # <Start> interact with the environment\n",
    "            old_obs = obs\n",
    "            action = policy(env, obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            states.append(obs)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            # <End> interact with the environment\n",
    "            \n",
    "            # <Start> Update state-values.\n",
    "            if lambda_ == 0:\n",
    "                # Monte Carlo.\n",
    "                if done:\n",
    "                    nr_steps = len(actions)\n",
    "                    for t in range(nr_steps):\n",
    "                        not_visited_yet = states[t] not in states[:t]\n",
    "                        if is_every_visit_mc or not_visited_yet:\n",
    "                            N[states[t]] += 1\n",
    "                            coeff = 1 / N[states[t]] if alpha is None else alpha\n",
    "                            G_t = sum(r * gamma ** i for i, r in enumerate(rewards[t+1:]))\n",
    "                            target = G_t\n",
    "                            V[old_obs] = V[old_obs] + coeff * (target - V[old_obs])  # running mean\n",
    "            elif lambda_ == 1:\n",
    "                # Temporal-Difference.\n",
    "                N[old_obs] += 1\n",
    "                coeff = 1 / N[old_obs] if alpha is None else alpha\n",
    "                target = reward + gamma * V[obs]\n",
    "                V[old_obs] = V[old_obs] + coeff * (target - V[old_obs])  # running mean\n",
    "            else:\n",
    "                raise ValueError('TD(lambda) not supported just yet.')\n",
    "            \n",
    "            \n",
    "        # Update state-value look-up table.\n",
    "        V_history[nr_episode] = V.copy()\n",
    "\n",
    "    return pd.DataFrame(V_history).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "monte_carlo_leaning(policy=policy_random, n_trajectories=5, is_every_visit_mc=True, gamma=1, alpha=None, lambda_=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD(λ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
